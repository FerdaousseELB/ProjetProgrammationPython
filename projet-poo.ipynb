{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16b68d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d67d7cb8364085a1419ce56d06af3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='corona', description='Subreddit:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef840220e5324499a6fd9dac2741f9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=100, description='Reddit Limit:', max=500, min=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a9aa08ce794b288ee8f69fc789e244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='corona', description='ArXiv Query Terms:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741f73a6ddc9431685c2b0209cd9cc99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=100, description='ArXiv Max Results:', max=500, min=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d78312eabeb45d187e8f79315e6c992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Collect and Display Data', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le fichier corpus_data.csv est crée avec 6 auteurs et 6 documents.\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from DataCollector import DataCollector\n",
    "from Author import Author\n",
    "from Document import Document\n",
    "from RedditDocument import RedditDocument\n",
    "from ArxivDocument import ArxivDocument\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def collect_data_and_display(subreddit, reddit_limit, arxiv_query_terms, arxiv_max_results):\n",
    "    # Créer une instance de DataCollector avec les paramètres fournis\n",
    "    collector = DataCollector(subreddit=subreddit_widget.value, limit=reddit_limit_widget.value,\n",
    "                          query_terms=arxiv_query_terms_widget.value.split(','), max_results=arxiv_max_results_widget.value)\n",
    "\n",
    "    # Collecter des données depuis Reddit\n",
    "    collector.collect_reddit_data()\n",
    "\n",
    "    # Collecter des données depuis ArXiv\n",
    "    collector.collect_arxiv_data()\n",
    "       \n",
    "    collection = []\n",
    "    for nature, doc in collector.docs_bruts:\n",
    "        if nature == \"ArXiv\": \n",
    "            titre = doc[\"title\"].replace('\\n', '')  # On enlève les retours à la ligne\n",
    "            try:\n",
    "                authors = \", \".join([a[\"name\"] for a in doc[\"author\"]])  # On fait une liste d'auteurs, séparés par une virgule\n",
    "            except:\n",
    "                authors = doc[\"author\"][\"name\"]  # Si l'auteur est seul, pas besoin de liste\n",
    "            summary = doc[\"summary\"].replace(\"\\n\", \"\")  # On enlève les retours à la ligne\n",
    "            date = datetime.datetime.strptime(doc[\"published\"], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y/%m/%d\")  # Formatage de la date en année/mois/jour avec librairie datetime\n",
    "\n",
    "            doc_classe = Document(titre, authors, date, doc[\"id\"], summary)  # Création du Document\n",
    "            collection.append(doc_classe)  # Ajout du Document à la liste.\n",
    "\n",
    "        elif nature == \"Reddit\":\n",
    "            titre = doc.title.replace(\"\\n\", '')\n",
    "            auteur = str(doc.author)\n",
    "            date = datetime.datetime.fromtimestamp(doc.created).strftime(\"%Y/%m/%d\")\n",
    "            url = str(doc.url)\n",
    "            texte = doc.selftext.replace(\"\\n\", \"\")\n",
    "            doc_classe = Document(titre, auteur, date, url, texte)\n",
    "            collection.append(doc_classe)\n",
    "    \n",
    "    authors = {}\n",
    "    aut2id = {}\n",
    "    num_auteurs_vus = 0\n",
    "    \n",
    "    for doc in collection:\n",
    "        if doc.auteur not in aut2id:\n",
    "            num_auteurs_vus += 1\n",
    "            authors[num_auteurs_vus] = Author(doc.auteur)\n",
    "            aut2id[doc.auteur] = num_auteurs_vus\n",
    "\n",
    "        authors[aut2id[doc.auteur]].add(doc.texte) \n",
    "    \n",
    "    corpus_data = {\n",
    "        'Titre': [],\n",
    "        'Auteur': [],\n",
    "        'Date': [],\n",
    "        'URL': [],\n",
    "        'Texte': []\n",
    "    }\n",
    "    \n",
    "    for doc in collection:\n",
    "        corpus_data['Titre'].append(doc.titre)\n",
    "        corpus_data['Auteur'].append(doc.auteur)\n",
    "        corpus_data['Date'].append(doc.date)\n",
    "        corpus_data['URL'].append(doc.url)\n",
    "        corpus_data['Texte'].append(doc.texte)\n",
    "        \n",
    "    corpus_df = pd.DataFrame(corpus_data)\n",
    "    corpus_df.to_csv('corpus_data.csv', sep='\\t', index=False)\n",
    "    \n",
    "    message = \"le fichier corpus_data.csv est crée avec {} auteurs et {} documents.\"\n",
    "    print(message.format(len(authors), len(collection)))\n",
    "\n",
    "# Créer les widgets pour le formulaire\n",
    "subreddit_widget = widgets.Text(value='corona', description='Subreddit:')\n",
    "reddit_limit_widget = widgets.IntSlider(value=100, min=1, max=500, step=1, description='Reddit Limit:')\n",
    "arxiv_query_terms_widget = widgets.Text(value='corona', description='ArXiv Query Terms:')\n",
    "arxiv_max_results_widget = widgets.IntSlider(value=100, min=1, max=500, step=1, description='ArXiv Max Results:')\n",
    "\n",
    "# Créer le bouton pour déclencher la collecte de données\n",
    "button = widgets.Button(description='Collect and Display Data')\n",
    "\n",
    "# Définir la fonction à exécuter lors du clic sur le bouton\n",
    "def on_button_click(b):\n",
    "    collect_data_and_display(subreddit_widget.value, reddit_limit_widget.value,\n",
    "                              arxiv_query_terms_widget.value.split(','), arxiv_max_results_widget.value)\n",
    "\n",
    "# Associer la fonction au clic sur le bouton\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "# Afficher les widgets\n",
    "display(subreddit_widget, reddit_limit_widget, arxiv_query_terms_widget, arxiv_max_results_widget, button)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed53e718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
